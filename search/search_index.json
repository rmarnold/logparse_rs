{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"logparse-rs SDK","text":"<p>High-performance, schema-driven log parsing and optional anonymization \u2014 Rust core with first-class Python bindings.</p> <p>This site provides comprehensive documentation for both: - Rust core crate: <code>logparse_core</code> - Python SDK package: <code>logparse-rs</code></p> <p>What you can do with logparse-rs: - Parse quote-aware CSV log lines reliably and fast (Rust memchr-accelerated) - Map CSV fields into named keys driven by a JSON schema (e.g., PAN-OS syslog) - Compute a stable 64-bit hash of the raw log line for correlation - Optionally anonymize selected fields with deterministic tokens or fixed values - Export an integrity table with original\u2192replacement mappings for audits</p> <p>If you\u2019re in a hurry, jump straight to Quickstart.</p>"},{"location":"#repository-layout","title":"Repository layout","text":"<pre><code>repo/\n  Cargo.toml                 # workspace\n  crates/\n    logparse_core/           # pure Rust core, publish to crates.io\n  bindings/\n    python/                  # PyO3 bindings, publish wheels to PyPI\n</code></pre>"},{"location":"#packages","title":"Packages","text":"<ul> <li>Rust crate: <code>logparse_core</code> (library)</li> <li>Python package: <code>logparse-rs</code> (binary extension built with PyO3)</li> </ul>"},{"location":"#status","title":"Status","text":"<ul> <li>Rust core: stable API surface for CSV split/extract, schema-driven parsing, FNV-1a hash, anonymizer primitives.</li> <li>Python bindings: stable for core flows; APIs may evolve prior to 1.0.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>This project follows semantic versioning when possible. Notable changes are documented here.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":"<ul> <li>Add MkDocs-based SDK documentation and GitHub Pages deployment workflow.</li> <li>Document Python API, schema format, anonymizer config, and helpers.</li> </ul>"},{"location":"changelog/#010","title":"0.1.0","text":"<ul> <li>Initial public release of <code>logparse_core</code> and <code>logparse-rs</code> with schema-driven parsing, CSV tokenizer, and anonymization primitives.</li> </ul>"},{"location":"env/","title":"Environment variables","text":"<p>The Python module attempts to preload schema/anonymizer based on environment variables at import time. This is useful for short-lived processes or serverless environments.</p> <p>Supported variables:</p> <ul> <li>LOGPARSE_PRELOAD_SCHEMA</li> <li>SCHEMA_JSON_PATH</li> <li>PAN_RUST_PRELOAD_SCHEMA</li> </ul> <p>If any of the above is set to a readable path, the schema is loaded into the process-wide cache on module import.</p> <ul> <li>LOGPARSE_ANON_CONFIG</li> <li>PAN_RUST_ANON_CONFIG</li> </ul> <p>If set to a readable path, an anonymizer configuration is loaded on module import.</p> <p>Example:</p> <pre><code>export LOGPARSE_PRELOAD_SCHEMA=/etc/logparse/schema.json\nexport LOGPARSE_ANON_CONFIG=/etc/logparse/anon.json\npython -c \"import logparse_rs, json; print(json.dumps(logparse_rs.get_schema_status()))\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#python-logparse-rs","title":"Python (logparse-rs)","text":"<p>Install the prebuilt wheel from PyPI (recommended):</p> <pre><code>pip install logparse-rs\n</code></pre> <p>Notes: - Requires Python 3.9\u20133.12 on Linux, macOS, or Windows. - Wheels are compiled with Rust/PyO3. - If a wheel is not available for your platform, <code>pip</code> may try to build from source; in that case you need a Rust toolchain (rustup) and maturin. Prefer a wheel if possible.</p> <p>Alternatively, install from a GitHub Release artifact (wheel or sdist): 1. Download a suitable wheel for your OS/Python version from the repository Releases. 2. Install with <code>pip install path/to/wheel.whl</code>.</p>"},{"location":"installation/#rust-logparse_core","title":"Rust (logparse_core)","text":"<p>Add the crate to your <code>Cargo.toml</code>:</p> <pre><code>cargo add logparse_core\n</code></pre> <p>Or manually:</p> <pre><code>[dependencies]\nlogparse_core = \"*\"\n</code></pre>"},{"location":"installation/#local-development","title":"Local development","text":"<ul> <li>Rust: Install the Rust toolchain via https://rustup.rs/</li> <li>Python: For building wheels locally use <code>maturin</code>:</li> </ul> <pre><code>pip install maturin\nmaturin develop -m bindings/python/Cargo.toml\n</code></pre> <p>This will build and install the Python extension into your current virtual environment for iterative development.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This page shows minimal, end-to-end examples for both Python and Rust.</p>"},{"location":"quickstart/#python","title":"Python","text":"<pre><code>import json\nimport logparse_rs as lp\n\n# 1) Load a schema (see the Schema page for format)\nassert lp.load_schema(\"schema.json\")\n\n# 2) Parse a CSV log line into a dict of fields\nline = \"2025/10/12 05:07:29,serial,THREAT,subtype,...\"\nparsed = lp.parse_kv(line)\nprint(parsed[\"type\"])  # e.g., \"THREAT\"\n\n# 3) Enriched parsing with metadata\nres = lp.parse_kv_enriched(line)\nprint(res[\"parsed\"][\"src_ip\"])  # if defined in your schema\nprint(res[\"hash64\"])             # 64-bit hash of the raw line\n\n# 4) Optional anonymization\nok = lp.load_anonymizer(\"anon.json\")\nres_anon = lp.parse_kv_enriched_anon(line)\nprint(res_anon[\"_anonymized\"])   # True\n\n# 5) Inspect current status\nprint(lp.get_schema_status())      # { \"loaded\": True, \"path\": ..., \"types\": N, ... }\nprint(lp.get_anonymizer_status())  # { \"enabled\": True, \"fields\": N, \"pairs\": M }\n</code></pre>"},{"location":"quickstart/#rust","title":"Rust","text":"<pre><code>use logparse_core::{load_schema_internal, parse_line_to_map};\n\nfn main() -&gt; Result&lt;(), String&gt; {\n    let schema = load_schema_internal(\"schema.json\")?;\n    let line = \"2025/10/12 05:07:29,serial,TRAFFIC,allow,...\";\n    let map = parse_line_to_map(line, &amp;schema)?;\n    println!(\"src={}, dst={}\",\n        map.get(\"src_ip\").and_then(|v| v.as_ref()).unwrap_or(\"-\"),\n        map.get(\"dst_ip\").and_then(|v| v.as_ref()).unwrap_or(\"-\"),\n    );\n    Ok(())\n}\n</code></pre> <p>Tips: - See Environment for preloading schema/anonymizer via env vars. - See Anonymization for config examples.</p>"},{"location":"rust/","title":"Rust core (<code>logparse_core</code>)","text":"<p>The Rust crate powers the parsing and anonymization. It\u2019s designed to be lightweight and easily embedded.</p> <ul> <li>Crate: https://crates.io/crates/logparse_core</li> <li>API docs: https://docs.rs/logparse_core/latest/logparse_core/</li> </ul>"},{"location":"rust/#public-api-highlights","title":"Public API highlights","text":"<ul> <li>schema</li> <li><code>load_schema_internal(path: &amp;str) -&gt; Result&lt;LoadedSchema, String&gt;</code></li> <li><code>ensure_schema_loaded(path: &amp;str) -&gt; Result&lt;(), String&gt;</code> and a global <code>SCHEMA_CACHE</code></li> <li>tokenizer</li> <li><code>split_csv_internal(line: &amp;str) -&gt; Vec&lt;String&gt;</code></li> <li><code>extract_field_internal(line: &amp;str, idx: usize) -&gt; Option&lt;String&gt;</code></li> <li>parser</li> <li><code>parse_line_to_map(line: &amp;str, schema: &amp;LoadedSchema) -&gt; Result&lt;HashMap&lt;String, Option&lt;String&gt;&gt;, String&gt;</code></li> <li>anonymizer</li> <li><code>anonymizer_from_json(json: &amp;str) -&gt; Result&lt;AnonymizerCore, String&gt;</code></li> <li><code>AnonymizerCore::anonymize_one(field, original) -&gt; Option&lt;String&gt;</code></li> </ul> <p>Utility: - <code>hash64_fnv1a(bytes: &amp;[u8]) -&gt; u64</code></p>"},{"location":"rust/#example","title":"Example","text":"<pre><code>use logparse_core::{load_schema_internal, parse_line_to_map};\n\nfn example() -&gt; Result&lt;(), String&gt; {\n    let schema = load_schema_internal(\"schema.json\")?;\n    let map = parse_line_to_map(\"x,y,z,TRAFFIC,sub,foo,bar,baz\", &amp;schema)?;\n    assert_eq!(map.get(\"f0\").and_then(|v| v.as_ref()), Some(&amp;\"x\".to_string()));\n    Ok(())\n}\n</code></pre> <p>For more details, see the docs.rs link above.</p>"},{"location":"schema/","title":"Schema format","text":"<p>The parser maps CSV fields into named keys using a JSON schema. The built-in example follows a Palo Alto Networks style, but the library is generic.</p> <p>Top-level shape:</p> <pre><code>{\n  \"palo_alto_syslog_fields\": {\n    \"log_types\": {\n      \"TRAFFIC\": {\n        \"type_value\": \"TRAFFIC\",\n        \"description\": \"Traffic logs\",\n        \"field_count\": 72,\n        \"fields\": [\n          \"time_generated\", \"serial\", \"type\", \"subtype\", \"src_ip\", \"dst_ip\"\n        ]\n      },\n      \"THREAT\": {\n        \"type_value\": \"THREAT\",\n        \"fields\": [\"time_generated\", \"serial\", \"type\", \"subtype\", \"misc\"]\n      }\n    }\n  }\n}\n</code></pre> <p>Rules: - <code>log_types</code> is a map of logical record types by name; each entry has:   - <code>type_value</code>: the literal string found in your CSV line that identifies the type (e.g., at index 3 in many PAN-OS logs)   - <code>fields</code>: list of field names in order (strings or objects <code>{ \"name\": \"...\" }</code>)   - optional <code>description</code> and <code>field_count</code> - Field names are sanitized:   - trimmed, lowercased, spaces and punctuation replaced with <code>_</code>   - must start with a letter or <code>_</code> \u2014 otherwise an <code>_</code> is prefixed</p> <p>Loader behavior: - On first load <code>load_schema(path)</code> parses the file and builds an in-memory mapping: <code>type_value -&gt; [field_names...]</code>. - <code>parse_kv*</code> extracts the type (at index 3 by convention), selects the field list for that type, splits the CSV line, and builds a dict. - Missing trailing fields are returned as <code>None</code>.</p> <p>Hot-reload semantics: - <code>parse_kv_with_schema(..., schema_path)</code> and <code>parse_kv_enriched_with_schema(..., schema_path)</code> call <code>ensure_schema_loaded</code>, which reloads when the file\u2019s mtime changes.</p> <p>Example minimal schema:</p> <pre><code>{\n  \"palo_alto_syslog_fields\": {\n    \"log_types\": {\n      \"TRAFFIC\": { \"type_value\": \"TRAFFIC\", \"fields\": [\"f0\", \"f1\", \"f2\", \"f3\"] }\n    }\n  }\n}\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#valueerror-no-schema-loaded","title":"ValueError: No schema loaded","text":"<p>Call <code>load_schema(path)</code> once before parsing, or use the <code>*_with_schema(...)</code> variants which will ensure the schema is loaded.</p>"},{"location":"troubleshooting/#unknown-log-type-in-schema-x","title":"Unknown log type in schema: X","text":"<p>Your CSV line contains a <code>type</code> field (commonly at index 3) whose value is not defined in your schema\u2019s <code>log_types</code>. Add an entry with matching <code>type_value</code> and corresponding field list.</p>"},{"location":"troubleshooting/#could-not-extract-log-type-at-index-3","title":"Could not extract log type at index 3","text":"<p>The parser expects the log type at a fixed index (3) for schema selection. Ensure your schema and input lines follow the same convention. You can still use <code>split_csv</code> and <code>extract_field</code> to inspect inputs.</p>"},{"location":"troubleshooting/#anonymizer-not-enabled","title":"Anonymizer not enabled","text":"<p>Load it with <code>load_anonymizer(path)</code> or <code>set_anonymizer_json(json_str)</code>. Use <code>get_anonymizer_status()</code> to verify.</p>"},{"location":"troubleshooting/#performance-considerations","title":"Performance considerations","text":"<ul> <li>Preload schema and anonymizer via environment variables to avoid per-process cold starts.</li> <li>Use <code>parse_kv_enriched</code> to get detailed timing (<code>parse_ns</code>, <code>anonymize_ns</code>) for profiling.</li> <li>Keep the anonymizer config lean; per-field maps are cached but very large maps can grow memory usage.</li> </ul>"},{"location":"python/","title":"Python SDK Overview","text":"<p>The <code>logparse-rs</code> Python package provides a fast, schema-driven log parser implemented in Rust and exposed via PyO3.</p> <p>Key capabilities: - Load a JSON schema and map CSV log lines into named fields - Robust CSV handling (quotes, embedded commas) - Enriched parse results with raw excerpt, 64-bit hash, and timing - Optional field anonymization with deterministic tokens or fixed replacements - Simple helpers for CSV-level operations (split a line, extract fields)</p> <p>Typical workflow: 1. <code>load_schema(path)</code> \u2014 load a schema once 2. <code>parse_kv(...)</code> or <code>parse_kv_enriched(...)</code> \u2014 parse lines at high throughput 3. Optional: <code>load_anonymizer(...)</code> \u2014 anonymize selected fields 4. Export <code>export_integrity_table()</code> for audit/compliance if needed</p> <p>Environment-based preload: - <code>LOGPARSE_PRELOAD_SCHEMA</code> or <code>SCHEMA_JSON_PATH</code> \u2014 preload schema on import - <code>LOGPARSE_ANON_CONFIG</code> \u2014 preload anonymizer config on import</p> <p>See the API Reference for the full list of functions.</p>"},{"location":"python/anonymizer/","title":"Anonymization","text":"<p>The anonymizer lets you replace sensitive field values deterministically while keeping analytical value.</p> <p>Supported modes per field: - tokenize (default): produce a stable token like <code>T_&lt;hash&gt;</code> - map: replace known values from a map, and control fallback behavior - fixed: always use a fixed replacement string</p> <p>Global defaults can be overridden per-field.</p>"},{"location":"python/anonymizer/#config-format","title":"Config format","text":"<p>JSON schema for the anonymizer configuration (version 1):</p> <pre><code>{\n  \"version\": 1,\n  \"defaults\": {\n    \"mode\": \"tokenize\",            \n    \"fixed\": null,                  \n    \"tokenize\": { \"prefix\": \"T_\", \"salt\": \"pepper\" }\n  },\n  \"fields\": {\n    \"username\": {\n      \"mode\": \"map\",\n      \"map\": { \"alice\": \"A\" },\n      \"fallback\": \"tokenize\",     \n      \"tokenize\": { \"prefix\": \"U_\" }\n    },\n    \"ip\": {\n      \"mode\": \"tokenize\",\n      \"tokenize\": { \"prefix\": \"IP_\" }\n    },\n    \"fixed_field\": {\n      \"mode\": \"fixed\",\n      \"fixed\": \"REDACTED\"\n    },\n    \"reject_field\": {\n      \"mode\": \"map\",\n      \"map\": {},\n      \"fallback\": \"reject\"\n    }\n  }\n}\n</code></pre> <p>Notes: - <code>tokenize.prefix</code> sets the token prefix; <code>salt</code> allows project-specific deterministic tokens. - <code>fallback</code> when <code>mode=map</code> decides behavior for unknown values: <code>tokenize</code> (default), <code>fixed</code>, or <code>reject</code>. - The anonymizer maintains an in-memory integrity table you can export.</p>"},{"location":"python/anonymizer/#python-usage","title":"Python usage","text":"<pre><code>import logparse_rs as lp\nlp.load_schema(\"schema.json\")\nlp.load_anonymizer(\"anon.json\")\n\nres = lp.parse_kv_enriched_anon(\"ts,serial,TRAFFIC,allow,10.0.0.1,10.0.0.2,...\")\nprint(res[\"_anonymized\"])     # True\nprint(res[\"parsed\"][\"src_ip\"]) # e.g., \"IP_...\" if configured\n\nstatus = lp.get_anonymizer_status()  # {\"enabled\": True, \"fields\": N, \"pairs\": M}\nitable = lp.export_integrity_table()  # {\"field\": {\"original\": \"replacement\", ...}}\n</code></pre> <p>Performance tips: - Load the anonymizer once and reuse. The integrity table grows lazily and ensures identical inputs map to identical outputs.</p>"},{"location":"python/api/","title":"Python API Reference","text":"<p>The <code>logparse_rs</code> module exposes the following functions. Import it as:</p> <pre><code>import logparse_rs as lp\n</code></pre>"},{"location":"python/api/#schema-driven-parsing","title":"Schema-driven parsing","text":"<ul> <li>load_schema(schema_path: str) -&gt; bool</li> <li> <p>Load a JSON schema from disk into a process-wide cache. Returns True on success; raises ValueError on error.</p> </li> <li> <p>parse_kv(line: str) -&gt; dict[str, Optional[str]]</p> </li> <li> <p>Parse one CSV log line into a dict of field_name -&gt; value (or None if missing). Requires a previously loaded schema.</p> </li> <li> <p>parse_kv_with_schema(line: str, schema_path: str) -&gt; dict[str, Optional[str]]</p> </li> <li> <p>Convenience method that ensures the given schema is loaded (reloads if changed) and parses the line in one call.</p> </li> <li> <p>parse_kv_enriched(line: str) -&gt; dict</p> </li> <li> <p>Like parse_kv, but returns a dict with:</p> <ul> <li>parsed: dict[str, Optional[str]] \u2014 the parsed fields</li> <li>raw_excerpt: str \u2014 up to the first 256 chars of the raw line</li> <li>hash64: int \u2014 64-bit FNV-1a hash of the raw line (as Python int)</li> <li>parse_ns: int \u2014 time spent parsing in nanoseconds</li> <li>runtime_ns_total: int \u2014 total runtime in nanoseconds</li> </ul> </li> <li> <p>parse_kv_enriched_with_schema(line: str, schema_path: str) -&gt; dict</p> </li> <li> <p>As above, but ensures the given schema is loaded.</p> </li> <li> <p>get_schema_status() -&gt; dict</p> </li> <li>Returns schema loader state, e.g., { \"loaded\": True/False, \"path\": str, \"types\": int }</li> </ul>"},{"location":"python/api/#csv-helpers","title":"CSV helpers","text":"<ul> <li>extract_field(line: str, index: int) -&gt; Optional[str]</li> <li> <p>Return the N-th field (0-based) from a CSV line, respecting quotes; None if out of bounds.</p> </li> <li> <p>extract_type_subtype(line: str) -&gt; tuple[Optional[str], Optional[str]]</p> </li> <li> <p>Convenience: returns the \"type\" and \"subtype\" fields commonly present in vendor logs. Both may be None.</p> </li> <li> <p>split_csv(line: str) -&gt; list[str]</p> </li> <li>Quote-aware fast splitter. All fields are returned as strings (may be empty strings).</li> </ul>"},{"location":"python/api/#anonymizer","title":"Anonymizer","text":"<ul> <li>load_anonymizer(config_path: str) -&gt; bool</li> <li> <p>Load anonymizer configuration from a JSON file. Returns True on success.</p> </li> <li> <p>set_anonymizer_json(config_json: str) -&gt; bool</p> </li> <li> <p>Load anonymizer configuration directly from a JSON string.</p> </li> <li> <p>get_anonymizer_status() -&gt; dict</p> </li> <li> <p>If enabled, returns { \"enabled\": True, \"fields\": N, \"pairs\": M } where pairs is the total integrity table size.</p> </li> <li> <p>export_integrity_table() -&gt; dict[str, dict[str, str]]</p> </li> <li> <p>Export the integrity table mapping: field -&gt; { original_value: replacement }. Useful for audits.</p> </li> <li> <p>parse_kv_enriched_anon(line: str) -&gt; dict</p> </li> <li> <p>Enriched parse with anonymization enabled (if config loaded). Adds <code>_anonymized: True</code> and <code>anonymize_ns</code> to timings.</p> </li> <li> <p>parse_kv_enriched_with_schema_anon(line: str, schema_path: str) -&gt; dict</p> </li> <li>Same as above, ensuring the given schema is loaded.</li> </ul>"},{"location":"python/api/#exceptions","title":"Exceptions","text":"<p>Most functions return simple booleans or dicts. Errors such as a missing schema surface as <code>ValueError</code> from Rust via PyO3.</p>"},{"location":"python/api/#example","title":"Example","text":"<pre><code>import logparse_rs as lp\nlp.load_schema(\"schema.json\")\nres = lp.parse_kv_enriched(\"ts,serial,TRAFFIC,allow,src,dst,...\")\nprint(res[\"parsed\"][\"src_ip\"])  # may be None if not present/defined by schema\n</code></pre>"},{"location":"python/csv_helpers/","title":"CSV Helpers","text":"<p>These helpers operate directly on CSV log lines and are useful to triage input quickly.</p>"},{"location":"python/csv_helpers/#split_csvline-str-liststr","title":"split_csv(line: str) -&gt; list[str]","text":"<p>Quote-aware CSV splitter. Handles embedded commas and quotes correctly, without allocating excessively.</p> <p>Example:</p> <pre><code>from logparse_rs import split_csv\nfields = split_csv('\"a,b\",c,,\"d\"')\nassert fields == [\"a,b\", \"c\", \"\", \"d\"]\n</code></pre>"},{"location":"python/csv_helpers/#extract_fieldline-str-index-int-optionalstr","title":"extract_field(line: str, index: int) -&gt; Optional[str]","text":"<p>Return the N-th field (0-based), or None if out of bounds.</p> <pre><code>from logparse_rs import extract_field\nprint(extract_field('1,2,3', 1))  # \"2\"\nprint(extract_field('1,2,3', 9))  # None\n</code></pre>"},{"location":"python/csv_helpers/#extract_type_subtypeline-str-tupleoptionalstr-optionalstr","title":"extract_type_subtype(line: str) -&gt; tuple[Optional[str], Optional[str]]","text":"<p>Convenience to extract the commonly used fields representing log type and subtype.</p> <pre><code>from logparse_rs import extract_type_subtype\nprint(extract_type_subtype('ts,serial,THREAT,spyware,...'))  # (\"THREAT\", \"spyware\")\n</code></pre>"}]}